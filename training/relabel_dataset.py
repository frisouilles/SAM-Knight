import pandas as pd
import re

# Fichier
DATASET_FILE = "training/stripchat_training_data.csv"

# --- LISTES DE MOTS-CLÃ‰S ---

# TOXIC : Insultes, hard, racisme, violence
TOXIC_KEYWORDS = [
    "pute", "salope", "chienne", "conne", "connasse", "grognasse", "pÃ©tasse", "truie", 
    "moche", "thon", "grosse", "vache", 
    "bite", "chatte", "queue", "zizi", "fesse", "cul", "anal", "sodomie", 
    "suce", "avale", "gorge", "bouche", "enculÃ©", "baise", "baiser", "levrette",
    "viol", "violer", "tuer", "meurtre", "crÃ¨ve", "suicide", "mort",
    "nÃ¨gre", "nÃ©gro", "bougnoule", "bicot", "youpin", "feuj", "pd", "pÃ©dale", "tapette", "gouine",
    "scato", "pisse", "urine", "caca", "merde",
    "bitch", "slut", "whore", "cunt", "fuck", "dick", "cock", "pussy", "asshole"
]

# NEUTRAL : Spam soft, rÃ©seaux sociaux, demandes privÃ©es, infos persos, fÃ©tichisme soft
NEUTRAL_KEYWORDS = [
    "snap", "insta", "whatsapp", "skype", "telegram", "kik", "facebook", "twitter", "onlyfans", "mym",
    "pv", "privÃ©", "prive", "message", "dm", "mp",
    "numÃ©ro", "numero", "tel", "tÃ©lÃ©phone", "telephone", "06", "07",
    "combien", "prix", "tarif", "coute", "coÃ»te", "payant", "gratuit", "free", "jetons", "tokens", "argent", "euros",
    "ville", "habites", "departement", "rÃ©gion", "pays", "d'oÃ¹", "dou", "viens tu",
    "age", "Ã¢ge", "vieille", "jeune", "mineur", "ecole", "lycÃ©e", "college",
    "montre", "voir", "fais voir", "enleve", "retire", "bas", "haut",
    "pieds", "feet", "aisselles", "nombril", "doigts", "mains",
    "rencontre", "rdv", "voir en vrai", "rÃ©el", "reel",
    "tu as un copain", "mari", "seule", "celibataire",
    "cam", "webcam", "show"
]

def normalize(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()
    # Remplacements basiques pour leetspeak
    text = text.replace("0", "o").replace("1", "i").replace("3", "e").replace("@", "a").replace("$", "s")
    return text

def classify_message(row):
    text = normalize(row['text'])
    original_label = row['label']
    
    # 1. VÃ©rification TOXIC (PrioritÃ© absolue)
    for word in TOXIC_KEYWORDS:
        # Recherche mot entier ou partiel significatif
        if word in text:
            # Petite exception pour Ã©viter les faux positifs (ex: "anal" dans "analyse" -> non, ici on fait simple)
            # Pour l'instant on bourrine un peu, c'est du pre-labeling
            return "toxic"
            
    # 2. VÃ©rification NEUTRAL
    for word in NEUTRAL_KEYWORDS:
        if word in text:
            return "neutral"
            
    # 3. Fallback : On garde l'original, sauf si c'Ã©tait "toxic" mais qu'on a rien trouvÃ© de flagrant
    # (Optionnel : on pourrait dÃ©grader "toxic" en "neutral" si pas de mot clÃ©, mais risquÃ©)
    # On va assumer que si c'Ã©tait marquÃ© "toxic" manuellement, Ã§a le reste.
    # Si c'Ã©tait "clean" et qu'on a rien trouvÃ©, Ã§a reste "clean".
    
    return original_label

def main():
    print(f"ðŸ”„ Chargement de {DATASET_FILE}...")
    try:
        df = pd.read_csv(DATASET_FILE)
    except FileNotFoundError:
        print("âŒ Fichier introuvable.")
        return

    print("ðŸ“Š Statistiques avant :")
    print(df['label'].value_counts())

    print("âš™ï¸ Re-classification en cours...")
    df['new_label'] = df.apply(classify_message, axis=1)
    
    # On compare
    changes = df[df['label'] != df['new_label']]
    print(f"ðŸ“ {len(changes)} labels modifiÃ©s.")
    
    if len(changes) > 0:
        print("\nExemples de changements :")
        print(changes[['text', 'label', 'new_label']].head(10))

    # Application
    df['label'] = df['new_label']
    df = df.drop(columns=['new_label'])

    # Sauvegarde
    df.to_csv(DATASET_FILE, index=False)
    print("\nâœ… Fichier sauvegardÃ© avec les nouveaux labels.")
    print("ðŸ“Š Statistiques aprÃ¨s :")
    print(df['label'].value_counts())

if __name__ == "__main__":
    main()
